[{"uri":"https://workshop-sample.fcjuni.com/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Do Thanh Long\nPhone Number: 0938036048\nEmail: thanhlong12092004@gmail.com\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” Event Objectives Introduce core concepts of AI/ML and GenAI on AWS Provide an overview of Amazon SageMaker and its end-to-end ML lifecycle Demonstrate Generative AI workflows using Amazon Bedrock Teach best practices in building, training, deploying, and scaling ML models on AWS Showcase real demos to help participants gain practical understanding Speakers (The event was presented by AWS Vietnam engineers and specialists.)\nAWS Solutions Architect Team – AWS Vietnam AI/ML Specialist Engineers – AWS SEA Generative AI Experts – Amazon Bedrock Team Key Highlights Understanding the AI/ML landscape Overview of AI/ML adoption trends in Vietnam and globally Business use cases adopting predictive models and GenAI The shift from traditional ML workflows to fully managed platforms (SageMaker) Amazon SageMaker – End-to-end ML platform Data preparation, feature engineering, labeling Model training, hyperparameter tuning, debugging Deployment options: real-time endpoints, batch, serverless inference Integrated MLOps capabilities Live Demo: SageMaker Studio notebook, pipelines, and model deployment Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan — strengths and use cases Prompt engineering: Few-shot Chain-of-thought Structured prompting Retrieval-Augmented Generation (RAG): Knowledge base Embeddings Vector search Bedrock Agents: building multi-step workflows Safety and guardrails for GenAI apps Live Demo: Build a GenAI chatbot using Amazon Bedrock Hands-on learning and discussions Real examples of GenAI in enterprise solutions How AI transforms software engineering workflows Best practices for scaling ML workloads with serverless and managed services Key Takeaways AI/ML and GenAI Knowledge Better understanding of the full ML lifecycle on AWS Awareness of when to apply Foundation Models vs traditional ML Improved prompt engineering skills Clear knowledge of RAG architecture for enterprise search/chatbots Technical Architecture Skills Learned how SageMaker unifies development, training, deployment, and monitoring Understood how to integrate Bedrock into backend applications Learned differences between real-time inference, serverless inference, and batch transforms Understood how to build safe, scalable GenAI applications with guardrails Modern AI Development Strategy Adopt MLOps to streamline ML development Leverage Bedrock Agents to automate multi-step workflows Use serverless and managed services to reduce operational overhead Apply best practices for monitoring ML endpoints and model drift Applying to Work Apply RAG architecture to enhance search/chatbot features in real applications Use prompt engineering techniques to improve model response quality Experiment with SageMaker Studio for future ML experiments Integrate Bedrock into backend services for GenAI-powered features Adopt scalable deployment patterns for ML-powered systems Event Experience Attending the “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” event was an eye-opening experience that expanded my understanding of modern AI/ML development. Key experiences included:\nLearning from experts AWS engineers shared practical best practices and real business case studies Learned how large organizations adopt ML pipelines and GenAI solutions Hands-on demonstrations Saw live demos of SageMaker training, deployment, and debugging Watched Bedrock building a full GenAI chatbot in real-time Understood how to integrate Foundation Models into applications via APIs Networking and discussions Interacted with AWS specialists and other developers Exchanged ideas on RAG, Bedrock Agents, and ML architectures Strengthened industry connections and developed soft skills Lessons learned Managed ML services significantly reduce development complexity RAG is essential for enterprise-grade GenAI applications Prompt engineering is a real skill that impacts model accuracy and safety AI workloads must be monitored and secured like any production system Event Photos Overall, this event enriched my technical knowledge and gave me practical insights into AI/ML and GenAI development on AWS. It also motivated me to explore how these technologies can be applied in future projects and real-world applications.\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 – DevOps on AWS” Event Objectives Provide an end-to-end understanding of DevOps practices on AWS Introduce CI/CD pipelines using AWS developer tools Demonstrate Infrastructure as Code (IaC) with CloudFormation and CDK Explore container services (ECR, ECS, EKS, App Runner) Teach monitoring, observability, and on-call best practices Share real-world DevOps case studies and transformation stories Speakers (Presented by AWS engineers and DevOps specialists in Vietnam and Southeast Asia)\nAWS Solutions Architect Team – DevOps Specialists AWS Container Services Experts AWS Observability and Cloud Operations Team Key Highlights DevOps Mindset \u0026amp; Principles (8:30–9:00 AM) Recap of AI/ML session from previous event Core DevOps cultural principles Understanding DORA metrics: Deployment Frequency Lead Time for Changes MTTR (Mean Time to Recovery) Change Failure Rate How DevOps improves agility, reliability, and team collaboration CI/CD Pipeline on AWS (9:00–10:30 AM) Source control with CodeCommit and recommended Git strategies (GitFlow, Trunk-based) Automated builds using CodeBuild with unit testing integration Deployment patterns with CodeDeploy: Blue/Green Canary Rolling updates CodePipeline orchestration for end-to-end automation Live Demo: Full CI/CD pipeline from source → build → deploy Infrastructure as Code (IaC) (10:45 AM–12:00 PM) CloudFormation fundamentals: templates, stacks, drift detection AWS CDK: constructs, reusable patterns, language support Live Demo: Deploying infrastructure using CloudFormation and CDK Comparison: When to choose CloudFormation vs CDK Afternoon Session (1:00–5:00 PM) Container Services on AWS (1:00–2:30 PM) Docker fundamentals and microservices packaging Amazon ECR: image storage, scanning, lifecycle policies Amazon ECS and EKS: Application orchestration Scaling strategies Deployment options AWS App Runner for simplified container deployment Case Study and Demo: Comparing ECS, EKS, and App Runner Monitoring \u0026amp; Observability (2:45–4:00 PM) CloudWatch: Metrics Logs Alarms Dashboards AWS X-Ray for distributed tracing and performance bottleneck analysis Demo: Setting up full-stack observability Best practices: alerting, dashboards, log retention, incident readiness DevOps Best Practices \u0026amp; Case Studies (4:00–4:45 PM) Deployment strategies: Feature flags A/B testing Automated testing in CI/CD pipelines Incident management and writing meaningful postmortems Case studies from startups to enterprise DevOps transformations Q\u0026amp;A \u0026amp; Wrap-up (4:45–5:00 PM) DevOps career pathways AWS certification roadmap (DevOps Engineer – Professional, SysOps Administrator) Tips for preparing real-world DevOps projects Key Takeaways DevOps Knowledge Clear understanding of DevOps culture, DORA metrics, and continuous improvement Confidence in designing and operating CI/CD pipelines on AWS Ability to choose the right deployment strategy depending on system requirements Technical Skills Hands-on knowledge of CodeCommit, CodeBuild, CodeDeploy, CodePipeline Practical understanding of IaC approaches with CloudFormation and CDK Stronger grasp of containerization and orchestration with ECS and EKS Familiarity with building observability stacks using CloudWatch and X-Ray Architecture \u0026amp; Operations Strategy Importance of automation for reliability and velocity How to build resilient pipelines and reduce MTTR Observability as a core part of operational excellence Understanding how DevOps practices scale from small teams to large enterprises Applying to Work Implement CI/CD pipeline for current or future backend projects Use IaC (CDK or CloudFormation) to manage AWS infrastructure declaratively Adopt container workflows using ECR plus ECS or EKS Set up CloudWatch dashboards and alerts for monitoring services Apply DevOps best practices such as feature flags, A/B testing, and automated testing Consider AWS DevOps certification pathway to advance career goals Event Experience Attending the AWS Cloud Mastery Series #2 – DevOps on AWS was highly impactful, giving me practical insights into both the cultural foundation and technical implementation of DevOps on AWS.\nLearning from experts AWS engineers presented real-world DevOps challenges and solutions Case studies demonstrated how organizations evolved their delivery pipelines Hands-on demonstrations Saw a full CI/CD pipeline built from scratch Observed container deployments using ECS, EKS, and App Runner Understood how monitoring and tracing work together for reliability Networking \u0026amp; discussions Engaged with AWS specialists and peers on DevOps tools and career pathways Shared problem-solving experiences, especially around deployments and on-call workflows Lessons learned Automation is the key to stability and high deployment frequency Observability is essential for operating modern distributed systems Containers and IaC are the foundation of scalable DevOps workflows Continuous improvement and postmortems lead to long-term team success Event Photos Overall, the event strengthened my understanding of DevOps on AWS and provided actionable strategies to apply these concepts to real-world projects.\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3 – Well-Architected Security Pillar” Event Objectives Provide an in-depth understanding of the AWS Well-Architected Security Pillar Introduce modern cloud security principles and real-world threat scenarios Demonstrate IAM best practices and multi-account governance Explain detection, monitoring, and automated alerting on AWS Explore infrastructure, network, and workload protection methods Teach data protection techniques including encryption and secrets management Walk through practical incident response workflows and automation Speakers (Presented by AWS Vietnam engineers and cloud security specialists)\nAWS Solutions Architect Team – Security Specialists AWS Security \u0026amp; Compliance Experts AWS Cloud Operations Team Key Highlights Opening \u0026amp; Security Foundation (8:30–8:50 AM) Importance of the Security Pillar in the Well-Architected Framework Core security principles: least privilege, zero trust, defense in depth Shared Responsibility Model and common misunderstandings Overview of the most common cloud security issues in Vietnam Pillar 1 – Identity \u0026amp; Access Management (IAM) (8:50–9:30 AM) IAM fundamentals: users, roles, temporary credentials, avoiding long-term keys IAM Identity Center (SSO): permission sets and workforce identity Multi-account governance with Organizations, SCP, permission boundaries Strengthening authentication: MFA, rotation policies, Access Analyzer Mini Demo: Validating IAM policies and simulating access Pillar 2 – Detection \u0026amp; Monitoring (9:30–9:55 AM) CloudTrail at organization level for centralized auditing Continuous threat detection with GuardDuty Security Hub: compliance checks and aggregated findings Logging at all layers: VPC Flow Logs ALB access logs S3 server access logs EventBridge for automated alerting and response Introduction to Detection-as-Code Coffee Break (9:55–10:10 AM) Pillar 3 – Infrastructure Protection (10:10–10:40 AM) VPC design best practices: segmentation, isolation, private connectivity Security Groups vs Network ACLs and where each fits Web protection with AWS WAF and DDoS protection with Shield Network-level defense using AWS Network Firewall Workload security fundamentals for EC2, ECS, and EKS Pillar 4 – Data Protection (10:40–11:10 AM) KMS fundamentals: key policies, grants, key rotation Encryption at rest and in transit across AWS services (S3, EBS, RDS, DynamoDB) Secrets lifecycle management using Secrets Manager and Parameter Store Data classification and guardrails for regulated workloads Pillar 5 – Incident Response (11:10–11:40 AM) AWS Incident Response lifecycle Sample IR playbooks: Compromised IAM access key Unintended S3 public exposure EC2 malware activity Handling incidents: isolation, evidence collection, snapshotting Automating IR using Lambda and Step Functions Wrap-Up \u0026amp; Q\u0026amp;A (11:40 AM–12:00 PM) Summary of all 5 security pillars Common security mistakes seen in real customer systems Recommended learning roadmap: Security Specialty, SA Pro Final questions and practical guidance from AWS specialists Key Takeaways Cloud Security Knowledge Deep understanding of AWS Well-Architected Security Pillar Clear view of modern cloud threat models and defense strategies Understanding of IAM governance and multi-account control mechanisms Technical Skills Hands-on familiarity with CloudTrail, GuardDuty, Security Hub, and EventBridge Ability to design secure VPC architectures with proper segmentation Practical knowledge of encryption, secret management, and KMS policies Exposure to real incident response workflows and automation patterns Security Architecture Mindset Importance of zero trust, least privilege, and defense in depth Applying layered detection and continuous monitoring Capability to assess security risks and design mitigation strategies Understanding how security integrates into all stages of cloud workloads Applying to Work Implement IAM best practices: temporary credentials, MFA, IAM Identity Center Enable centralized CloudTrail and GuardDuty for monitoring and threat detection Apply VPC segmentation and correct use of Security Groups and NACLs Enforce encryption everywhere using KMS and implement secrets rotation Build automated alerts and incident response workflows using EventBridge and Lambda Adopt AWS Well-Architected Security Pillar as a checklist during architecture reviews Strengthen incident readiness with playbooks and snapshot-based evidence collection Event Experience Attending the AWS Cloud Mastery Series #3 – Well-Architected Security Pillar provided valuable insights into modern cloud security and real-world AWS best practices.\nLearning from experts AWS specialists shared real customer scenarios and common misconfigurations Clear explanations of how enterprises build secure multi-account environments Hands-on demonstrations Policy simulation and IAM validation Centralized logging and threat detection workflows Examples of automated incident response using serverless technologies Networking \u0026amp; discussions Discussed cloud security pitfalls with AWS engineers and other attendees Learned practical tips on preparing for AWS Security Specialty certification Lessons learned Security must be continuous, automated, and embedded into every stage Monitoring and detection are essential to cloud defense Strong IAM foundation is the backbone of all AWS security Incident response should be automated to minimize impact and reduce MTTR Event Photos Overall, the event strengthened my cloud security mindset and gave me the tools, frameworks, and practical knowledge to design secure and resilient workloads on AWS.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Set up a personal AWS Free Tier account and complete initial security configurations. Become familiar with the AWS Management Console and how to navigate AWS services. Understand the fundamental AWS service groups (Compute, Storage, Networking, IAM). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Create AWS Free Tier account - Enable MFA, set billing alarms, and review security best practices 08/11/2025 08/11/2025 https://aws.amazon.com/getting-started 2 - Explore AWS Management Console interface - Learn how to search, access, and navigate AWS services 08/12/2025 08/12/2025 https://aws.amazon.com/console 3 - Learn basic AWS service groups: + Compute + Storage + Networking + IAM + Global Infrastructure 08/12/2025 08/12/2025 https://aws.amazon.com/what-is-aws 4 - Practice with S3: + Create S3 bucket + Upload objects + Test object access permissions 08/13/2025 08/13/2025 https://docs.aws.amazon.com/s3 5 - Learn EC2 basics: + Instance types + Key pair + Security groups + EBS + Elastic IP 08/14/2025 08/14/2025 https://docs.aws.amazon.com/ec2 6 - Practice: + Launch EC2 instance + Connect via SSH using key pair + Start/stop instance 08/15/2025 08/15/2025 https://docs.aws.amazon.com/ec2 Week 1 Achievements: Created and set up a personal AWS Free Tier account with proper security configurations (MFA, billing alerts). Became familiar with the AWS Management Console UI and learned how to access and use core AWS services. Gained foundational knowledge of AWS service groups: Compute (EC2 overview) Storage (S3 concepts) Networking (VPC basics, regions vs. availability zones) IAM (users, roles, permissions) Practiced using Amazon S3: created buckets, uploaded files, managed access permissions. Learned EC2 fundamentals including instance types, EBS volumes, key pairs, and security groups. Successfully launched an EC2 instance and connected through SSH for the first time. Improved understanding of how AWS resources are organized and how services integrate with each other. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Strengthen knowledge of AWS through deeper exploration of core services. Learn AWS Networking fundamentals (VPC, Subnet, Routing, IGW). Practice configuring S3, IAM, and EC2 with more advanced features. Gain hands-on experience building basic cloud infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Review Week 1 knowledge (EC2, S3, IAM) - Learn deeper about Security Groups \u0026amp; key pair management 08/18/2025 08/18/2025 https://docs.aws.amazon.com/ec2 2 - Study AWS VPC fundamentals: + VPC + Subnets + Route Tables + Internet Gateway + NAT Gateway 08/19/2025 08/19/2025 https://docs.aws.amazon.com/vpc 3 - Practice: Build a custom VPC + Create public \u0026amp; private subnets + Configure routing tables + Attach an Internet Gateway 08/20/2025 08/20/2025 https://cloudjourney.awsstudygroup.com 4 - Learn advanced S3 concepts: + S3 Versioning + Storage Classes + Lifecycle Rules + SSE-S3 \u0026amp; SSE-KMS encryption 08/21/2025 08/21/2025 https://docs.aws.amazon.com/s3 5 - Practice: + Enable S3 versioning + Create lifecycle rule + Configure bucket policy \u0026amp; block public access 08/22/2025 08/22/2025 https://docs.aws.amazon.com/s3 6 - Learn IAM roles \u0026amp; policies + Managed \u0026amp; inline policies + IAM role trust policy + Permission boundaries 08/23/2025 08/23/2025 https://docs.aws.amazon.com/iam Week 2 Achievements: Strengthened understanding of AWS EC2 security groups, key pairs, and networking behavior. Successfully created a custom VPC with subnets, routing tables, and an Internet Gateway. Practiced designing a basic network architecture in AWS using VPC components. Explored Amazon S3 advanced features such as versioning, lifecycle management, and encryption. Configured access control for S3 using bucket policies and block public access rules. Improved IAM knowledge by learning how roles, trust policies, and permission models work. Gained confidence in navigating AWS services and linking them together to build cloud infrastructure. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn how AWS handles scaling and high availability. Understand Elastic Load Balancing (ELB) and Auto Scaling Groups (ASG). Explore CloudWatch metrics, alarms, and logs for monitoring AWS resources. Build and test a scalable web architecture using EC2 + ALB + ASG. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Elastic Load Balancing concepts + Application Load Balancer (ALB) + Listeners \u0026amp; Target Groups + Health checks 08/25/2025 08/25/2025 https://docs.aws.amazon.com/elasticloadbalancing 2 - Practice: + Deploy an ALB + Add EC2 instances to Target Group + Configure health checks \u0026amp; test load distribution 08/26/2025 08/26/2025 https://cloudjourney.awsstudygroup.com 3 - Study Auto Scaling Groups (ASG): + Launch Templates + Scaling Policies + Desired/Min/Max capacity 08/27/2025 08/27/2025 https://docs.aws.amazon.com/autoscaling 4 - Practice: + Create ASG with Launch Template + Attach ASG to ALB + Test auto scale-out \u0026amp; scale-in behavior 08/28/2025 08/28/2025 https://docs.aws.amazon.com/autoscaling 5 - Learn CloudWatch Monitoring: + Metrics + Dashboards + CloudWatch Alarms + EC2 \u0026amp; ASG monitoring 08/29/2025 08/29/2025 https://docs.aws.amazon.com/cloudwatch 6 - Practice: + Create CPU utilization alarm + Attach alarm to ASG scaling policy + Review CloudWatch Logs 08/30/2025 08/30/2025 https://cloudjourney.awsstudygroup.com Week 3 Achievements: Understood how AWS Elastic Load Balancer distributes traffic and improves system availability. Successfully deployed an Application Load Balancer and configured listeners, target groups, and health checks. Learned how Auto Scaling Groups maintain application scalability and resilience. Created a Launch Template and configured Auto Scaling with dynamic scaling policies. Tested scale-in and scale-out actions using CloudWatch CPU alarms. Explored CloudWatch features such as metrics, dashboards, alarms, and logs to monitor system performance. Built a complete scalable architecture combining EC2 + ALB + ASG + CloudWatch. Gained practical experience in designing fault-tolerant workloads on AWS. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn the fundamentals of serverless computing on AWS. Understand AWS Lambda execution model, triggers, permissions, and runtime. Explore API Gateway for building RESTful APIs. Learn DynamoDB concepts: partitions, primary keys, and read/write capacity. Build and deploy a basic serverless application. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn AWS Lambda fundamentals + Execution model + Runtimes + IAM roles for Lambda 09/01/2025 09/01/2025 https://docs.aws.amazon.com/lambda 2 - Practice: Create first Lambda function + Test using AWS Console + Review logs in CloudWatch 09/02/2025 09/02/2025 https://docs.aws.amazon.com/lambda 3 - Learn API Gateway basics + REST API vs HTTP API + Methods, routes, stages 09/03/2025 09/03/2025 https://docs.aws.amazon.com/apigateway 4 - Practice: Build REST API with API Gateway + Integrate API Gateway → Lambda + Deploy \u0026amp; test via Postman 09/04/2025 09/04/2025 https://cloudjourney.awsstudygroup.com 5 - Learn DynamoDB fundamentals + Partition key \u0026amp; sort key + Items \u0026amp; attributes + RCU / WCU 09/05/2025 09/05/2025 https://docs.aws.amazon.com/dynamodb 6 - Practice: + Create DynamoDB Table + Lambda → DynamoDB integration + Implement simple CRUD logic 09/06/2025 09/06/2025 https://docs.aws.amazon.com/dynamodb Week 4 Achievements: Learned how serverless architecture works and how Lambda manages compute automatically. Built and tested multiple Lambda functions and monitored them using CloudWatch Logs. Created a RESTful API using API Gateway and successfully integrated it with Lambda. Understood DynamoDB table design, partition keys, sort keys, throughput capacity, and best practices. Built a simple serverless CRUD workflow using: API Gateway (API routing) Lambda (function logic) DynamoDB (NoSQL database) Strengthened the ability to build lightweight, event-driven, highly scalable backend systems on AWS. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn the fundamentals of containerization on AWS. Understand how Amazon ECR stores and manages container images. Explore Amazon ECS and its components: tasks, services, clusters. Deploy a containerized application using ECS Fargate (serverless compute). Strengthen skills in modern cloud-native application deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn basics of containers \u0026amp; Docker + Images vs Containers + Dockerfile + Container registries 09/08/2025 09/08/2025 https://docs.docker.com 2 - Learn Amazon ECR concepts + Repositories + Image push/pull + Permissions \u0026amp; auth 09/09/2025 09/09/2025 https://docs.aws.amazon.com/ecr 3 - Practice: + Build Docker image + Tag \u0026amp; push image to ECR 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com 4 - Learn Amazon ECS fundamentals + Clusters + Task Definitions + ECS Services + Launch types: EC2 vs Fargate 09/11/2025 09/11/2025 https://docs.aws.amazon.com/ecs 5 - Practice: Deploy a containerized app on ECS Fargate + Create cluster + Create task definition + Run service pulling image from ECR 09/12/2025 09/12/2025 https://docs.aws.amazon.com/ecs 6 - Learn ECS networking + Security groups + Subnets \u0026amp; ENIs + Load balancing with ALB + ECS 09/13/2025 09/13/2025 https://docs.aws.amazon.com/elasticloadbalancing Week 5 Achievements: Built a strong foundation in Docker containerization and the process of building and tagging images. Successfully stored and managed container images in Amazon ECR. Understood ECS architecture and the relationship between clusters, tasks, and services. Deployed a fully functional containerized application using ECS Fargate (serverless container compute). Learned how ECS integrates with VPC networking, security groups, and Application Load Balancer. Strengthened cloud-native deployment skills using AWS managed container services. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand DevOps principles and CI/CD concepts on AWS. Learn how AWS CodePipeline automates software delivery workflows. Explore CodeBuild for building and testing source code. Learn CodeDeploy deployment strategies (EC2, ECS). Build and test a complete CI/CD pipeline. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn CI/CD fundamentals + Continuous Integration + Continuous Delivery \u0026amp; Deployment + Deployment strategies 09/15/2025 09/15/2025 https://aws.amazon.com/devops/ 2 - Study AWS CodePipeline concepts + Stages + Actions + Artifacts 09/16/2025 09/16/2025 https://docs.aws.amazon.com/codepipeline 3 - Learn CodeBuild: + Buildspec.yml + Build environments + Logs \u0026amp; artifacts 09/17/2025 09/17/2025 https://docs.aws.amazon.com/codebuild 4 - Practice: Create CodeBuild project + Build \u0026amp; test sample source code + Store build artifacts 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com 5 - Learn CodeDeploy: + AppSpec.yml + Deployment groups + Deployment revisions 09/19/2025 09/19/2025 https://docs.aws.amazon.com/codedeploy 6 - Practice: + Build CI/CD pipeline using CodePipeline + Integrate CodeBuild \u0026amp; CodeDeploy + Deploy sample application automatically 09/20/2025 09/20/2025 https://docs.aws.amazon.com/codepipeline Week 6 Achievements: Gained a clear understanding of CI/CD workflows and modern DevOps practices. Successfully created a CodeBuild project using buildspec.yml to automate application builds. Learned how deployment strategies work in CodeDeploy, including rolling update and blue/green deployment. Built a complete CI/CD pipeline using: CodePipeline (orchestration) CodeBuild (build phase) CodeDeploy (deployment) Automated the entire process from source → build → deploy. Strengthened knowledge of automation, release management, and DevOps tooling on AWS. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand the fundamentals of Infrastructure as Code (IaC). Learn AWS CloudFormation concepts: templates, stacks, resources. Explore YAML/JSON CloudFormation template structure. Practice deploying infrastructure using CloudFormation. Learn how to update, delete, and troubleshoot CloudFormation stacks. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Infrastructure as Code concepts + Benefits of IaC + Declarative vs Imperative IaC 09/22/2025 09/22/2025 https://aws.amazon.com/what-is/iac/ 2 - Study CloudFormation basics: + Stacks + Templates + Resources, Parameters, Outputs 09/23/2025 09/23/2025 https://docs.aws.amazon.com/cloudformation 3 - Understand CloudFormation template structure + AWS::Resource types + Intrinsic functions (Ref, Fn::GetAtt, etc.) 09/24/2025 09/24/2025 https://docs.aws.amazon.com/AWSCloudFormation/latest 4 - Practice: Deploy simple stack + S3 bucket + IAM role + EC2 instance 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com 5 - Learn stack updates: + Change sets + Update policies + Rollback behavior 09/26/2025 09/26/2025 https://docs.aws.amazon.com/cloudformation 6 - Practice: + Modify existing stack + Add new resources + Handle failed updates \u0026amp; rollback 09/27/2025 09/27/2025 https://docs.aws.amazon.com/cloudformation Week 7 Achievements: Built a strong understanding of Infrastructure as Code and its importance in cloud automation. Learned the structure and components of AWS CloudFormation templates. Successfully deployed CloudFormation stacks containing S3, IAM, and EC2 resources. Practiced updating stacks using change sets and understood rollback mechanisms. Learned to use intrinsic functions such as Ref and Fn::GetAtt within templates. Strengthened automation skills by provisioning infrastructure declaratively using IaC. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Strengthen knowledge of AWS monitoring \u0026amp; observability tools. Learn CloudWatch Logs, Metrics, Dashboards \u0026amp; Alarms in depth. Understand CloudTrail for auditing and API activity tracking. Learn AWS Config for configuration tracking \u0026amp; compliance. Build a monitoring and auditing system for cloud resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Deep dive into CloudWatch Metrics + EC2 metrics + Custom metrics + Dashboards 09/29/2025 09/29/2025 https://docs.aws.amazon.com/cloudwatch 2 - Practice: Create CloudWatch dashboard + Add EC2 CPU/Network graphs + Add S3 \u0026amp; Lambda metrics 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com 3 - Learn CloudWatch Logs \u0026amp; Log Insights + Log groups/streams + Queries with CloudWatch Logs Insights 10/01/2025 10/01/2025 https://docs.aws.amazon.com/cloudwatch 4 - Study CloudTrail: + Event history + Trails + Management vs Data events + Security auditing 10/02/2025 10/02/2025 https://docs.aws.amazon.com/cloudtrail 5 - Learn AWS Config: + Configuration items + Rules + Timeline \u0026amp; drift detection 10/03/2025 10/03/2025 https://docs.aws.amazon.com/config 6 - Practice: + Enable AWS Config rules + Detect non-compliant resources + Review CloudTrail logs for API activity 10/04/2025 10/04/2025 https://docs.aws.amazon.com/config Week 8 Achievements: Built a deeper understanding of CloudWatch Metrics and how they reflect system performance. Created CloudWatch dashboards to visualize EC2, S3, and Lambda service metrics. Gained experience working with CloudWatch Logs and querying logs using Log Insights. Understood how CloudTrail records API calls and how it’s used for auditing and security review. Enabled AWS Config to track configuration changes and detect non-compliant resources. Improved skills in monitoring, logging, auditing, and maintaining observability in cloud environments. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn relational database services on AWS. Understand Amazon RDS architecture, backups, Multi-AZ, and maintenance. Explore Amazon Aurora and its performance/scalability benefits. Learn how to connect applications to RDS and manage database security. Deploy, configure, and test a real RDS database instance. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn RDS fundamentals + DB instances + Parameter \u0026amp; option groups + Multi-AZ vs Read Replicas 10/06/2025 10/06/2025 https://docs.aws.amazon.com/rds 2 - Study RDS security: + Subnet groups + Security groups + Authentication \u0026amp; encryption 10/07/2025 10/07/2025 https://docs.aws.amazon.com/rds 3 - Practice: Create an RDS MySQL/PostgreSQL instance + Configure subnet group + Connect via client tool 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com 4 - Learn Aurora fundamentals + Cluster structure + Writer \u0026amp; Reader endpoints + Storage autoscaling 10/09/2025 10/09/2025 https://docs.aws.amazon.com/aurora 5 - Compare RDS vs Aurora + Performance + Fault tolerance + Cost considerations 10/10/2025 10/10/2025 https://aws.amazon.com/rds/aurora/ 6 - Practice: + Deploy Aurora Serverless v2 cluster + Test failover between endpoints + Monitor DB performance in CloudWatch 10/11/2025 10/11/2025 https://docs.aws.amazon.com/aurora Week 9 Achievements: Understood how Amazon RDS provides managed relational database services. Configured VPC subnet groups, security groups, and DB parameters. Successfully deployed and connected to an RDS instance using MySQL/PostgreSQL client. Learned Aurora cluster architecture and how writer/reader endpoints work in high availability setups. Compared RDS and Aurora to understand performance, scaling, and cost differences. Gained hands-on experience monitoring database performance using CloudWatch metrics. Strengthened ability to choose and configure relational database solutions for cloud applications. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Setting Up AWS Account \u0026amp; Exploring Core Services\nWeek 2: Strengthening AWS Fundamentals \u0026amp; Hands-on with Core Services\nWeek 3: Scaling, Load Balancing \u0026amp; Monitoring on AWS\nWeek 4: Serverless with Lambda, API Gateway \u0026amp; DynamoDB\nWeek 5: Containers with ECS, ECR \u0026amp; Fargate\nWeek 6: CI/CD Pipeline with CodePipeline, CodeBuild \u0026amp; CodeDeploy\nWeek 7: CloudFormation \u0026amp; Infrastructure as Code (IaC)\nWeek 8: Monitoring, Logging \u0026amp; Observability with CloudWatch, CloudTrail \u0026amp; AWS Config\nWeek 9: RDS, Aurora \u0026amp; SQL Databases on AWS\nWeek 10: Project Ideation\nWeek 11: API Design, Use Cases \u0026amp; Backend Architecture\nWeek 12: Project Completion, System Testing \u0026amp; Final Delivery\n"},{"uri":"https://workshop-sample.fcjuni.com/2-proposal/","title":"Proposal ","tags":[],"description":"","content":"Video Sharing Platform 1.Executive Summary This proposal outlines the development of a scalable Video Sharing Platform leveraging AWS cloud services. The platform will enable users to upload, stream, and share video content with features including user authentication, content management, and real-time video streaming.\nKey objectives:\nBuild a secure, scalable video sharing platform Implement user authentication and authorization Provide high-quality video streaming capabilities Ensure cost-effective infrastructure management Deliver seamless user experience across devices The solution utilizes AWS services including Amplify for frontend hosting, Cognito for authentication, S3 for storage, CloudFront for content delivery, and Interactive Video Service for streaming capabilities.\n2.Problem Statement What\u0026rsquo;s the Problem? Current video sharing solutions face several challenges:\nHigh infrastructure costs for video storage and streaming Complex setup and maintenance requirements Limited scalability during peak usage Security vulnerabilities in user authentication Poor video quality and buffering issues Lack of real-time analytics and monitoring The Solution Our AWS-based video sharing platform addresses these challenges by:\nLeveraging AWS\u0026rsquo;s cost-effective, pay-as-you-use pricing model Utilizing managed services to reduce operational overhead Implementing auto-scaling capabilities for handling traffic spikes Providing enterprise-grade security through AWS Cognito and WAF Delivering high-quality video streaming via Amazon IVS and CloudFront Offering comprehensive monitoring and analytics through CloudWatch Benefits and Return on Investment Cost Savings:\n40-60% reduction in infrastructure costs compared to traditional hosting No upfront hardware investments required Pay-per-use model optimizes operational expenses Performance Improvements:\n99.9% uptime availability Global content delivery with low latency Auto-scaling handles 10x traffic increases seamlessly Business Value:\nFaster time-to-market (3-6 months vs 12+ months) Enhanced user experience drives higher engagement Scalable architecture supports business growth 3.Solution Architecture AWS Services Used Route 53: DNS service for domain management and traffic routing with health checks and failover capabilities.\nAmplify: Frontend hosting and deployment platform for React/Vue.js applications with CI/CD integration.\nCognito: User authentication and authorization service providing secure sign-up, sign-in, and access control.\nWAF: Web Application Firewall protecting against common web exploits and DDoS attacks.\nApp Runner: Containerized backend API hosting with automatic scaling and load balancing.\nDynamoDB: NoSQL database for storing user profiles, video metadata, and application data.\nS3: Object storage for video files, thumbnails, and static assets with versioning and lifecycle policies.\nCloudFront: Global CDN for fast content delivery and video streaming with edge caching.\nAmazon IVS (Interactive Video Service): Real-time video streaming service for live broadcasts and on-demand content with low latency.\nCloudWatch: Monitoring and logging service for application performance, metrics, and alerts.\nCode Pipeline: CI/CD pipeline for automated testing, building, and deployment.\nCode Build: Build service for compiling source code, running tests, and creating deployment packages.\nElastic Container Registry: Docker container registry for storing and managing application images.\nComponent Design Frontend Layer:\nReact-based web application hosted on Amplify Responsive design supporting mobile and desktop Real-time video player with adaptive bitrate streaming API Layer:\nRESTful APIs built with Node.js/Express Containerized and deployed on App Runner JWT-based authentication integration Data Layer:\nDynamoDB tables for user data and video metadata S3 buckets for video storage with intelligent tiering ElastiCache for session management and caching Security Layer:\nCognito user pools for authentication WAF rules for application protection IAM roles and policies for access control Streaming Architecture:\nAmazon IVS for live streaming capabilities CloudFront for global video distribution Adaptive bitrate streaming for optimal quality Monitoring \u0026amp; Analytics:\nCloudWatch dashboards for real-time metrics Custom metrics for user engagement tracking Automated alerting for system health Use Cases Live Streaming Events:\nReal-time broadcasting of conferences, webinars, and corporate events Multi-bitrate streaming for optimal viewer experience Video On Demand (VOD):\nUpload and share educational content, tutorials, and training materials Secure content access with user permissions Social Video Sharing:\nUser-generated content sharing Community features with comments and ratings 4.Technical Implementation Phase 1: Infrastructure Setup AWS Account Configuration:\nSet up AWS Organizations for multi-account management Configure IAM roles and policies for least privilege access Establish VPC with public/private subnets across multiple AZs Core Services Deployment:\nDeploy DynamoDB tables with proper indexing Configure S3 buckets with encryption and lifecycle policies Set up Cognito user pools and identity pools Configure Route 53 hosted zones and health checks Phase 2: Backend Development API Development:\nBuild RESTful APIs using Node.js/Express framework Implement JWT authentication with Cognito integration Create video upload/processing endpoints Develop user management and content APIs Database Schema:\nUsers table: user_id, email, profile_data, created_at Videos table: video_id, user_id, metadata, upload_status Analytics table: event_id, user_id, video_id, timestamp, action Containerization:\nCreate Docker containers for API services Push images to Elastic Container Registry Configure App Runner for automatic deployment Phase 3: Frontend Development React Application:\nImplement responsive UI components Integrate AWS Amplify SDK for authentication Build video upload interface with progress tracking Create video player with adaptive streaming Key Features:\nUser registration/login with email verification Video upload with drag-and-drop functionality Real-time video streaming with quality selection User dashboard for content management Phase 4: Streaming Integration Amazon IVS Setup:\nConfigure streaming channels and playback URLs Implement adaptive bitrate streaming Set up recording and archival workflows CloudFront Configuration:\nCreate distributions for video content delivery Configure edge locations for global reach Implement caching strategies for optimal performance Phase 5: Security \u0026amp; Monitoring Security Implementation:\nDeploy WAF with custom rules for protection Configure SSL/TLS certificates via Certificate Manager Implement API rate limiting and throttling Monitoring Setup:\nCreate CloudWatch dashboards for system metrics Set up alarms for critical performance indicators Implement logging for audit and troubleshooting Phase 6: CI/CD Pipeline Automated Deployment:\nConfigure CodePipeline for source-to-production workflow Set up CodeBuild for automated testing and building Implement blue-green deployment strategy Testing Strategy:\nUnit tests for API endpoints Integration tests for AWS service interactions Load testing for performance validation 5.Timeline \u0026amp; Milestones Project Duration: 8 Weeks (2 Months) Week 1: Setup \u0026amp; Planning\nAWS account setup and IAM configuration Project requirements finalization Team roles assignment Basic infrastructure deployment (S3, DynamoDB, Cognito) Week 2-3: Backend Development\nRESTful APIs with Node.js/Express JWT authentication with Cognito Video upload endpoints Database schemas implementation App Runner deployment Week 4-5: Frontend Development\nReact application with responsive design User authentication flows Video upload interface Basic video player Amplify deployment Week 6: Integration \u0026amp; Streaming\nFrontend-backend integration CloudFront setup for video delivery Basic streaming functionality Testing and bug fixes Week 7: Security \u0026amp; Testing\nWAF deployment SSL/TLS certificates Security testing Performance optimization Load testing Week 8: Final Deployment\nProduction deployment User acceptance testing Documentation completion Project presentation preparation Key Milestones Milestone 1 (Week 1): Infrastructure Ready\nAWS services configured Development environment accessible Milestone 2 (Week 3): Backend Complete\nAPIs functional Authentication working Milestone 3 (Week 5): Frontend Complete\nUI fully developed Basic video upload/playback working Milestone 4 (Week 8): Production Launch\nSystem deployed and tested Documentation complete 6.Budget Estimation Monthly Operating Costs (USD) Compute Services:\nApp Runner (1 services): $5-15/month Amplify Hosting: $0-5/month Storage \u0026amp; Database:\nS3 Storage: $0-1/month DynamoDB: $0-2/month CloudFront Data Transfer: $0-2/month Streaming Services:\nAmazon IVS (100 hours/month): $150-300/month Video Processing: $20-50/month Security \u0026amp; Monitoring:\nWAF: $5-10/month CloudWatch: $0-3/month Cognito: $0/month Networking:\nRoute 53: $0.5/month CI/CD\nCodePipeline \u0026amp; CodeBuild: $1-3/month\nERC: $0-1/month\nCalculator\nTotal Monthly Cost: $17-42/month\n7.Risks Assessment Primary Risks Technical Risks:\nStudents unfamiliar with AWS services → Training and workshops Integration complexity → Start simple, gradually increase Time management → Build buffer time, prioritize core features Resource Risks:\nExceeding AWS Free Tier → Monitor usage, set up alerts Team varying skill levels → Pair programming, mentorship Academic schedule conflicts → Flexible planning Mitigation Solutions Technical Management:\nCloudFormation templates Phase-by-phase testing Dev/staging environments Contingency Plans:\nMVP: Basic video upload/playback Core: User auth + streaming Advanced: Live streaming (optional) Use AWS Educate credits Mock services for demos 8.Expected Outcomes Performance Metrics System Performance:\nVideo upload success rate: \u0026gt;95% Streaming latency: \u0026lt;3 seconds System uptime: \u0026gt;99% Concurrent users: 100+ Page load times: \u0026lt;2 seconds Success Criteria MVP Requirements:\nUser registration/login Basic video upload/playback Secure authentication Responsive interface System monitoring Stretch Goals:\nLive streaming capabilities Advanced analytics Social features Mobile companion app "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Finalize the project idea: Cloud-based Video Sharing \u0026amp; Livestream Platform. Research reference platforms and analyze their core workflows. Define system requirements \u0026amp; MVP scope. Design the full DynamoDB database schema for the project. Draw the ERD \u0026amp; relationship diagram for system entities. Produce a refined version of the technical architecture and proposal. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Brainstorm \u0026amp; validate project idea + Video upload \u0026amp; playback + Livestream + Social interactions 10/13/2025 10/13/2025 Internal notes 2 - Research YouTube architecture \u0026amp; AWS media services + Ingest → Storage → Transcode → Playback workflow 10/14/2025 10/14/2025 AWS Media Services Docs 3 - Define feature set (MVP) + Video upload \u0026amp; streaming + User channels + Likes, comments, subscriptions + Notifications 10/15/2025 10/15/2025 Project notebook 4 - Design DynamoDB schema: + Identify entities (Users, Channels, Videos, Comments, Likes…) + Choose PK/SK for each table + Design GSIs for queries + One-to-many \u0026amp; many-to-many handling 10/16/2025 10/16/2025 DynamoDB Design Best Practices 5 - Draw ERD / NoSQL relationship diagram: + Channel → Videos + User → Subscriptions + Video → Comments + Video → Likes + Livestream Sessions 10/17/2025 10/17/2025 diagrams.net 6 - Update project proposal (v2): + Architecture diagram + DynamoDB schema overview + ERD \u0026amp; core workflows + Data flow (Upload → MediaConvert → CloudFront) 10/18/2025 10/18/2025 Internal proposal template Week 10 Achievements: Finalized the Video Platform idea and confirmed all necessary MVP features. Studied content delivery workflows used in modern media platforms. Clearly defined project scope including: Authentication Channel management Video upload \u0026amp; HLS output playback Likes, comments, subscriptions Notifications Livestream (IVS) workflow Designed a complete DynamoDB database schema, including: Users Table\nPK: UserId Channels Table\nPK: ChannelId, attributes: OwnerUserId Videos Table\nPK: VideoId, GSI for ChannelId Comments Table\nPK: VideoId, SK: CommentId Likes Table\nPK: VideoId, SK: UserId Subscriptions Table\nPK: UserId, SK: ChannelId Notifications Table\nPK: UserId, SK: NotificationId LivestreamSessions Table\nPK: StreamId, GSI for ChannelId Completed ERD / Relationship Diagram, showing: One-to-many: Channel → Videos One-to-many: Video → Comments Many-to-many: Users ↔ Channels (Subscriptions) Many-to-many: Users ↔ Videos (Likes) One-to-many: User → LivestreamSessions Updated the architecture diagram to include: S3 (raw + processed) MediaConvert CloudFront for CDN Cognito for auth DynamoDB for NoSQL data API Gateway / .NET Web API backend EventBridge + Lambda orchestration Prepared Project Proposal v2 including architecture, database design, diagrams, and feature breakdown. "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Transform system requirements into detailed API specifications. Write full use-case flows for the main features. Design backend architecture using Clean Architecture (.NET 8). Define all endpoints, request/response DTOs \u0026amp; controller structure. Finalize DynamoDB table access patterns and integrate them into API design. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Define detailed use cases for MVP: + User registration \u0026amp; login + Channel creation + Video upload flow + Comment/Like/Subscribe interactions 10/20/2025 10/20/2025 Project notes 2 - Design API endpoints: + Routes (RESTful format) + HTTP methods + Authentication requirements 10/21/2025 10/21/2025 API design guidelines 3 - Define request/response DTOs: + User DTO + Video DTO + Channel DTO + Comment DTO + Subscription DTO 10/22/2025 10/22/2025 Internal API Doc 4 - Backend Architecture Design (.NET): + Project structure (API, Application, Domain, Infrastructure) + Dependency injection + Service interfaces \u0026amp; implementations 10/23/2025 10/23/2025 Clean Architecture templates 5 - Map DynamoDB access patterns → API flows: + PK/SK lookups + Query patterns + GSI usage for listing videos, comments, etc. 10/24/2025 10/24/2025 DynamoDB docs 6 - Draft full technical specification document: + Use case diagrams + API list + ERD/DynamoDB schema + Architecture diagram 10/25/2025 10/25/2025 diagrams.net Week 11 Achievements: Completed detailed use-case flows for all MVP features including: User authentication (Cognito) Video upload \u0026amp; processing via S3 + MediaConvert Comment, Like, Subscribe flows Channel management Notifications \u0026amp; livestream workflows Designed a complete REST API specification including: Full endpoint list HTTP methods \u0026amp; request/response format Authentication requirements Error handling plan Built the initial backend architecture using Clean Architecture: Domain layer (entities, value objects) Application layer (services, interfaces, DTOs, validators) Infrastructure layer (DynamoDB repositories, AWS SDK integration) API layer (controllers, exception middleware) Completed mapping between DynamoDB access patterns and API logic: Query Videos by Channel (GSI) Get Comments by Video (PK = VideoId) Check Subscription status (PK/SK) Count Likes (Query by VideoId) Produced a technical specification v1 including: Use case diagram ERD (DynamoDB relationship diagram) Backend architecture diagram API list \u0026amp; data flow documentation "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete full implementation of the Video Sharing \u0026amp; Livestream Platform. Finalize all backend modules: Authentication, Channels, Videos, Comments, Likes, Subscriptions, Notifications, Livestream. Integrate AWS services end-to-end (S3, MediaConvert, CloudFront, DynamoDB, Cognito, IVS). Complete functional testing, integration testing, API testing, and end-to-end testing. Prepare deployment build, documentation, and final demo materials. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Complete implementation of remaining backend modules + Notifications + Livestream Session tracking + Final API adjustments 10/27/2025 10/27/2025 Project codebase 2 - Integrate AWS services end-to-end: + Upload → S3 raw + Trigger → MediaConvert + Output → S3 processed + Delivery → CloudFront 10/28/2025 10/28/2025 AWS docs 3 - Database validation on DynamoDB: + Users / Channels / Videos + Comments / Likes / Subscriptions + StreamSessions \u0026amp; Notifications 10/29/2025 10/29/2025 DynamoDB schema 4 - Perform testing: + API testing (Postman) + Integration testing + End-to-end feature testing 10/30/2025 10/30/2025 Postman workspace 5 - Fix bugs and refine UX flows + Video playback issues + API timing + Race conditions on MediaConvert triggers 10/31/2025 10/31/2025 Debug logs 6 - Prepare final deliverables: + System documentation + Deployment guide + Test report + Demo script + Submit final project 11/01/2025 11/01/2025 Internal checklist Week 12 Achievements: Completed and fully tested the entire Video Platform system from end to end. All backend modules finalized and working together: Authentication (Cognito) Users \u0026amp; Channels Video upload → transcoding → playback Comments, likes, subscriptions Notification system Livestream (IVS + event tracking) All DynamoDB tables validated with correct PK/SK and GSIs. Completed API testing, integration testing, and full system QA. Verified MediaConvert workflow and CloudFront delivery of HLS videos. Prepared complete documentation and delivered the final project package. System is ready for mentor review and final presentation. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 – AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS Event Name: AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS Date \u0026amp; Time: 08:30 AM – 12:00 PM, November 15, 2025 (Saturday)\nLocation: AWS Vietnam Office\nBitexco Financial Tower, 2 Hải Triều, Bến Nghé Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent Description This workshop was part of the AWS Cloud Mastery Series, focusing on Artificial Intelligence (AI), Machine Learning (ML), and Generative AI (GenAI) capabilities on AWS.\nThe session covered an end-to-end overview of modern AI workflows and demonstrated how AWS services accelerate the development, deployment, and scaling of machine learning applications.\nThe event consisted of three main parts:\n1. Welcome \u0026amp; Introduction (08:30 – 09:00) Registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 2. AWS AI/ML Services Overview (09:00 – 10:30) Introduction to Amazon SageMaker Data preparation and labeling Model training, tuning, and deployment Integrated MLOps functionalities Live Demo: SageMaker Studio walkthrough 3. Generative AI with Amazon Bedrock (10:45 – 12:00) Overview of Foundation Models (Claude, Llama, Titan) Prompt engineering techniques Chain-of-thought reasoning and few-shot learning RAG (Retrieval-Augmented Generation) architecture Bedrock Agents for multi-step workflows Safety and guardrails for GenAI Live Demo: Build a Generative AI chatbot using Bedrock Outcomes / Value Gained Gained a strong understanding of AWS AI/ML ecosystem, especially SageMaker workflows. Learned how Generative AI services (Bedrock) can be integrated into real applications. Improved prompt engineering skills and understanding of RAG architecture. Understood best practices for deploying secure and scalable AI workloads on AWS. Enhanced soft skills through networking with AWS engineers and other participants. Received event materials and gifts, contributing to a memorable learning experience. Event 2 – AWS Cloud Mastery Series #2 – DevOps on AWS Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS Date \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025 (GMT+7)\nLocation: Bitexco Financial Tower\n2 Hải Triều Street, Bến Nghé Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent Description This workshop focused on DevOps culture, CI/CD practices, infrastructure automation, container services, and observability on AWS.\nThe event covered the full DevOps lifecycle through presentations, demos, and case studies.\nMorning Session (8:30 AM – 12:00 PM) 1. Welcome \u0026amp; DevOps Mindset (8:30 – 9:00 AM) Recap of previous AI/ML session DevOps culture and principles Key DevOps benefits Introduction to DORA metrics (MTTR, deployment frequency, lead time) 2. AWS DevOps Services – CI/CD Pipeline (9:00 – 10:30 AM) Source control with AWS CodeCommit Git workflows: GitFlow, Trunk-based development Build \u0026amp; Test with CodeBuild Deployment strategies using CodeDeploy: Blue/Green Canary Rolling updates Pipeline orchestration with CodePipeline Demo: End-to-end CI/CD pipeline execution 3. Break (10:30 – 10:45 AM) 4. Infrastructure as Code (IaC) (10:45 AM – 12:00 PM) AWS CloudFormation essentials Stack management, drift detection AWS CDK overview: Constructs, reusable patterns, multilang support Demo: Deploying applications using CloudFormation \u0026amp; CDK Discussion: When to use CDK vs CloudFormation Lunch Break (12:00 – 1:00 PM) – Self-arranged Afternoon Session (1:00 PM – 5:00 PM) 5. Container Services on AWS (1:00 – 2:30 PM) Docker fundamentals and microservice packaging Amazon ECR: Image storage, scanning, lifecycle rules Amazon ECS \u0026amp; EKS: Cluster management Deployment strategies Auto scaling AWS App Runner for simplified container deployment Demo \u0026amp; Case Study: Comparing microservice deployment models 6. Break (2:30 – 2:45 PM) 7. Monitoring \u0026amp; Observability (2:45 – 4:00 PM) CloudWatch metrics, logs, dashboards Alarms and operational visibility AWS X-Ray for distributed tracing Demo: Complete observability setup Best practices for alerting, dashboards, and incident management 8. DevOps Best Practices \u0026amp; Case Studies (4:00 – 4:45 PM) Deployment testing: Feature flags, A/B testing Automated testing strategies and CI integration Incident response and postmortems Real-world DevOps transformations in startups and enterprises 9. Q\u0026amp;A \u0026amp; Wrap-up (4:45 – 5:00 PM) DevOps career roadmap Relevant AWS certifications Final discussion and knowledge sharing Outcomes / Value Gained Gained a complete picture of modern DevOps practices on AWS Learned how to build automated CI/CD pipelines using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Understood infrastructure automation using CloudFormation and AWS CDK Improved understanding of container orchestration with ECS, EKS, and App Runner Learned essential monitoring and observability tools (CloudWatch, X-Ray) Enhanced understanding of deployment strategies (blue/green, canary, rolling) Strengthened knowledge of incident management, postmortems, and DevOps best practices Connected with AWS engineers and fellow developers for knowledge exchange Event 3 – AWS Cloud Mastery Series #3 – Well-Architected Security Pillar Event Name: AWS Cloud Mastery Series #3 – Well-Architected Framework: Security Pillar Date \u0026amp; Time: 08:30 AM – 12:00 PM, Saturday, November 29, 2025 (GMT+7)\nLocation: AWS Vietnam Office\nBitexco Financial Tower\n2 Hải Triều Street, Bến Nghé Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent Description This workshop focused on the Security Pillar of the AWS Well-Architected Framework.\nThe session provided a practical understanding of modern cloud security, including IAM, detection, infrastructure protection, data protection, and incident response.\nThe event followed the official Well-Architected Security Pillar structure.\nOpening \u0026amp; Security Foundation (8:30 – 8:50 AM) Role of Security Pillar within the Well-Architected Framework Core security principles: least privilege, zero trust, defense-in-depth Shared Responsibility Model in the cloud Common cloud security threats in Vietnam How organizations adopt a layered security model Pillar 1 — Identity \u0026amp; Access Management (8:50 – 9:30 AM) IAM fundamentals: users, roles, policies, avoiding long-term credentials IAM Identity Center: single sign-on and permission sets Service Control Policies and permission boundaries for multi-account setups Strong authentication practices: MFA, rotation, Access Analyzer Mini Demo: Policy validation and access simulation Pillar 2 — Detection (9:30 – 9:55 AM) Continuous monitoring using CloudTrail (organization-level) Threat detection with GuardDuty Aggregated security posture with Security Hub Logging across all layers: VPC Flow Logs, ALB logs, S3 access logs Alert automation via EventBridge Introduction to Detection-as-Code Coffee Break (9:55 – 10:10 AM) Pillar 3 — Infrastructure Protection (10:10 – 10:40 AM) VPC segmentation and network placement (private vs public) Security Groups vs NACLs and recommended use cases Layer 7 and Layer 3 protection with WAF, Shield, and Network Firewall Workload security basics across EC2, ECS, and EKS Recommended isolation and traffic control patterns Pillar 4 — Data Protection (10:40 – 11:10 AM) Encryption fundamentals and KMS key policies, grants, and rotation Data-at-rest and data-in-transit encryption across S3, EBS, RDS, DynamoDB Secrets Manager and Parameter Store for secrets lifecycle Data classification, tagging, and guardrail enforcement How organizations ensure compliance with data governance requirements Pillar 5 — Incident Response (11:10 – 11:40 AM) AWS Incident Response lifecycle Sample playbooks for common cloud incidents: Compromised IAM key Public S3 bucket exposure EC2 malware or anomaly detection Techniques: snapshots, isolation, evidence collection Automated incident response using Lambda and Step Functions Wrap-Up \u0026amp; Q\u0026amp;A (11:40 AM – 12:00 PM) Summary of all 5 security pillars Common pitfalls in cloud security for Vietnamese enterprises Roadmap for AWS security learning (Security Specialty, SA Pro) Outcomes / Value Gained Deep understanding of the AWS Well-Architected Security Pillar Ability to design and evaluate secure architectures following best practices Knowledge of modern IAM strategies and organization-wide security controls Hands-on experience with detection, logging, and continuous monitoring Clear understanding of encryption models and secrets management Practical exposure to automated incident response patterns Awareness of common cloud misconfigurations and how to avoid them Strengthened security mindset applicable to production workloads "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://workshop-sample.fcjuni.com/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services (AWS) Vietnam) from September 2025 to November 2025, I applied my cloud and backend knowledge to build a real-world Video Sharing \u0026amp; Livestream Platform using .NET 8 Web API, Clean Architecture, and various AWS Services such as S3, MediaConvert, DynamoDB, Lambda, CloudFront, and IVS.\nThroughout this project, I improved significantly in areas such as system design, cloud architecture, problem-solving, API development, and team collaboration.\nI was responsible for implementing many core features of the platform, including:\nVideo upload → S3 → MediaConvert transcoding Automatic unsafe-content detection using Amazon Rekognition Channel management and subscription system Like, comment (with image upload), and notification workflow Livestream management using Amazon IVS + EventBridge + Lambda API development under Clean Architecture with MediatR, FluentValidation, and ProblemDetails Deploying backend on Azure and frontend on Vercel I always strived to complete tasks with quality, followed coding standards, communicated clearly with teammates, and proactively studied AWS best practices to improve the project.\nTo evaluate my progress objectively, I provide the following assessment:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applied cloud concepts; built APIs; integrated S3, MediaConvert, DynamoDB, IVS effectively ✅ ☐ ☐ 2 Ability to learn Quickly learned new AWS services and adapted to complex cloud scenarios ✅ ☐ ☐ 3 Proactiveness Proactively researched AWS docs and explored new design patterns ☐ ✅ ☐ 4 Sense of responsibility Delivered assigned tasks on time and ensured code quality ✅ ☐ ☐ 5 Discipline Occasionally inconsistent study schedule ☐ ☐ ☐ 6 Progressive mindset Accepted feedback and improved code structure and cloud workflows ✅ ☐ ☐ 7 Communication Communicated well with teammates; improving technical explanation ☐ ✅ ☐ 8 Teamwork Participated actively in team discussions and supported members ✅ ☐ ☐ 9 Professional conduct Maintained professionalism and followed AWS work culture ✅ ☐ ☐ 10 Problem-solving skills Solved issues in transcoding, IVS events, and S3 object access ☐ ✅ ☐ 11 Contribution to project/team Added meaningful features and improved overall cloud integration ✅ ☐ ☐ 12 Overall Demonstrated technical growth and strong commitment ✅ ☐ ☐ Needs Improvement Improve discipline and maintain a more consistent self-learning schedule Strengthen problem-solving for complex cloud architectures Enhance communication skills, especially in explaining system design and reporting "},{"uri":"https://workshop-sample.fcjuni.com/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Below is my personal sharing about the experience during my internship at AWS Vietnam as part of the First Cloud Journey program. These insights may help the FCJ team further enhance the program for future participants.\nOverall Evaluation 1. Working Environment\nThe working environment at AWS Vietnam is highly professional yet friendly. Everyone is open to helping whenever I face technical challenges, especially with AWS services that I have never used before. The workspace is clean, comfortable, and equipped with everything needed for learning and development. I felt welcomed and supported from the first day, and the collaborative culture helped me quickly adapt to the team.\n2. Support from Mentor / Team Admin\nMy mentor provided strong technical guidance, especially in AWS cloud architecture, serverless workflows, and best practices. Whenever I encountered issues with S3, MediaConvert, or IVS, the mentor gave hints rather than the final answer, helping me develop independence and problem-solving skills.\nThe admin team was also very supportive with onboarding, documents, schedules, and ensuring that interns had everything needed to focus on learning.\n3. Relevance of Work to Academic Major\nThe project I worked on—building a Video Sharing \u0026amp; Livestream Platform using .NET 8, AWS Serverless, DynamoDB, and event-driven architecture—was directly related to my major in software engineering. It allowed me to apply what I learned at university while exploring real-world cloud technologies not covered in class. This significantly strengthened both my theoretical and practical understanding.\n4. Learning \u0026amp; Skill Development Opportunities\nThis internship offered tremendous learning opportunities. I gained hands-on experience with many AWS services (S3, Lambda, MediaConvert, Rekognition, IVS, DynamoDB), Clean Architecture, and API development. I also improved my skills in debugging, documentation, code standards, and cloud design.\nThrough mentorship and workshops, I learned about scalability, security, cost-optimization, and modern development workflows—valuable skills that I will carry into future projects.\n5. Company Culture \u0026amp; Team Spirit\nThe culture at AWS Vietnam is positive, supportive, and growth-oriented. Everyone values respect, collaboration, and ownership. Even though I was an intern, I felt like a true member of the team. When someone faced a technical blocker, others were always willing to step in and help. This created a motivating and welcoming learning environment.\n6. Internship Policies / Benefits\nThe internship offered flexible working hours, an allowance, and access to AWS learning resources. I also appreciated the weekly sharing sessions, internal learning materials, and guidance related to AWS certifications. These benefits created a balanced learning environment that supported both productivity and personal development.\nAdditional Questions • What did you find most satisfying during your internship?\nI was most satisfied with the opportunity to build a real-world cloud project from scratch using AWS services. Seeing the video platform and livestream features working end-to-end was an extremely rewarding experience. I also enjoyed how supportive and friendly the mentors and team members were.\n• What should the company improve for future interns?\nIt would be helpful to have more team-bonding activities or small group sharing sessions among interns so we can learn from one another’s progress and experiences.\n• Would you recommend this internship to a friend? Why or why not?\nYes, definitely. The program provides hands-on cloud experience, real AWS mentorship, and a project that is directly applicable to industry. It’s an excellent opportunity for anyone who wants to strengthen both backend and cloud skills.\nSuggestions \u0026amp; Expectations I suggest adding more internal workshops focused on hands-on cloud labs or best practices for specific AWS services. I would be willing to continue participating in future programs if given the opportunity. Overall, I am extremely grateful for this internship and the guidance from the FCJ and AWS Vietnam team. The experience has helped me grow both technically and professionally. "},{"uri":"https://workshop-sample.fcjuni.com/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://workshop-sample.fcjuni.com/tags/","title":"Tags","tags":[],"description":"","content":""}]